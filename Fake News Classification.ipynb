{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d5c1ae",
   "metadata": {},
   "source": [
    "# Fake News Classification with the help of NLP Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79828e27",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384fe53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdb39",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401fd77",
   "metadata": {},
   "source": [
    "*Steps to Import the Dataset Correctly\n",
    "1]Use the Raw URL: To read the CSV file directly, you need to use the raw content URL rather than the GitHub page URL. The raw URL for the dataset can be constructed by replacing github.com with raw.githubusercontent.com and removing the blob/ part.\n",
    "\n",
    "2]Read the CSV File: Use pd.read_csv() with the corrected URL.\n",
    "\n",
    "3]Additional Considerations\n",
    "Check for Delimiters: If the CSV file uses a different delimiter (like a semicolon ; instead of a comma ,), you can specify the delimiter in the read_csv function using the sep parameter:\n",
    "python\n",
    "df = pd.read_csv(url, sep=';')\n",
    "\n",
    "4]Inspect the Dataset: If you continue to encounter issues, consider downloading the dataset manually and inspecting it to understand its structure. This can help identify any irregularities in the data format.\n",
    "\n",
    "5]Handling Bad Lines: If there are problematic lines in the CSV, you can use the error_bad_lines parameter (deprecated in newer versions, use on_bad_lines instead):\n",
    "python\n",
    "df = pd.read_csv(url, on_bad_lines='skip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a51fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              title  \\\n",
      "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2   2                  Why the Truth Might Get You Fired   \n",
      "3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4   4  Iranian woman jailed for fictional unpublished...   \n",
      "5   5  Jackie Mason: Hollywood Would Love Trump if He...   \n",
      "6   6  Life: Life Of Luxury: Elton John’s 6 Favorite ...   \n",
      "7   7  Benoît Hamon Wins French Socialist Party’s Pre...   \n",
      "8   8  Excerpts From a Draft Script for Donald Trump’...   \n",
      "9   9  A Back-Channel Plan for Ukraine and Russia, Co...   \n",
      "\n",
      "                         author  \\\n",
      "0                 Darrell Lucus   \n",
      "1               Daniel J. Flynn   \n",
      "2            Consortiumnews.com   \n",
      "3               Jessica Purkiss   \n",
      "4                Howard Portnoy   \n",
      "5               Daniel Nussbaum   \n",
      "6                           NaN   \n",
      "7               Alissa J. Rubin   \n",
      "8                           NaN   \n",
      "9  Megan Twohey and Scott Shane   \n",
      "\n",
      "                                                text  label  \n",
      "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
      "1  Ever get the feeling your life circles the rou...      0  \n",
      "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
      "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
      "4  Print \\nAn Iranian woman has been sentenced to...      1  \n",
      "5  In these trying times, Jackie Mason is the Voi...      0  \n",
      "6  Ever wonder how Britain’s most iconic pop pian...      1  \n",
      "7  PARIS  —   France chose an idealistic, traditi...      0  \n",
      "8  Donald J. Trump is scheduled to make a highly ...      0  \n",
      "9  A week before Michael T. Flynn resigned as nat...      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the raw URL\n",
    "url = 'https://raw.githubusercontent.com/AI-ML-LEARNING/Fake-News-Classification_NLP/main/News_dataset.csv'\n",
    "\n",
    "# Attempt to read the CSV file\n",
    "try:\n",
    "    df = pd.read_csv(url)\n",
    "    print(df.head(10))\n",
    "except pd.errors.ParserError as e:\n",
    "    print(\"ParserError:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a3d4d",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a613c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae70cb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    10413\n",
       "0    10387\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691682f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32be327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b80ab61",
   "metadata": {},
   "source": [
    "*if the data set has numerical features or column with missing value they can be replaced by mean,median and mode\n",
    "\n",
    "*but if dataset  is categorical(like yes,no,true,flase) in that case we replace with mode values\n",
    "\n",
    "*but the dataset that i used is text(NLP),most of the time available data is text,so we cannot replace with mode or anything.In   that case we need to drop those columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4fee7",
   "metadata": {},
   "source": [
    "##### Handeled missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47521875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()   #this drops the missing value rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8752904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05bb01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee5b8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id                                              title  \\\n",
       "0      0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1      1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2      2   2                  Why the Truth Might Get You Fired   \n",
       "3      3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4      4   4  Iranian woman jailed for fictional unpublished...   \n",
       "\n",
       "               author                                               text  \\\n",
       "0       Darrell Lucus  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1     Daniel J. Flynn  Ever get the feeling your life circles the rou...   \n",
       "2  Consortiumnews.com  Why the Truth Might Get You Fired October 29, ...   \n",
       "3     Jessica Purkiss  Videos 15 Civilians Killed In Single US Airstr...   \n",
       "4      Howard Portnoy  Print \\nAn Iranian woman has been sentenced to...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74db050",
   "metadata": {},
   "source": [
    "df.reset_index(inplace=True)\n",
    "This line resets the index of the DataFrame df. By default, when you create a DataFrame, it assigns an integer index starting from 0. However, if you have performed any operations that changed the index, such as filtering or grouping, this line will reset the index to a new sequence of integers.\n",
    "The inplace=True parameter tells the reset_index() function to modify the DataFrame in place, rather than creating a new DataFrame. This means that the original DataFrame df will be updated with the new index.\n",
    "After running this line, the DataFrame df will have a new index, and any previous index will be added as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df9a15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text is too much.For now we will work on title\n",
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8a18d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              title  label\n",
       "0      0  House Dem Aide: We Didn’t Even See Comey’s Let...      1\n",
       "1      1  FLYNN: Hillary Clinton, Big Woman on Campus - ...      0\n",
       "2      2                  Why the Truth Might Get You Fired      1\n",
       "3      3  15 Civilians Killed In Single US Airstrike Hav...      1\n",
       "4      4  Iranian woman jailed for fictional unpublished...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['id','text','author'],axis = 1)  #axis 1 means columns and 0 means rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c5019",
   "metadata": {},
   "source": [
    "### Data  Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93e615",
   "metadata": {},
   "source": [
    "Repairing and cleaning the test data for machines to be abel to analyze it. This puts data in workable form and highlights features in the text that an algorithm can work on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59262d7d",
   "metadata": {},
   "source": [
    "##### 1.Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc0197",
   "metadata": {},
   "source": [
    "Token helps in understanding the context,also they help in interpreting the meaning of the text by analyzing the sequence of words\n",
    "\n",
    "Two types of tokenization: word tokenization and sentence tokenization.\n",
    "\n",
    "So here we see **word tokenization**.This is used to break the sentence into the separate words or tokens/ it is the process of splitting a string into a text or list of tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ca0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take one sample data\n",
    "sample_data = 'The quick brown fox jumps over the lazy dog'\n",
    "sample_data = sample_data.split()\n",
    "print(sample_data)\n",
    "len(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288b6de",
   "metadata": {},
   "source": [
    "##### 2.Make Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954c6be",
   "metadata": {},
   "source": [
    "Say for egs in our sample data, \"the is repeated 2 times\",one with T(capital) and t(small).\n",
    "\n",
    "So if we go for next preprocessing without doing any alterations(maybe convert text data into numerics) so 2 different feature is created.Word is same but one  T(capital) is used and t(small) is used,that is why 2 separate feature is created..\n",
    "\n",
    "Or sometimes there might be an typo in our dataset (the,The,THe,etc),here 3 different feature is created\n",
    "\n",
    "To avoid this all are converted to small letter.Basically we use **list compherension** technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad49f4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = [data.lower() for data in sample_data]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9965ca",
   "metadata": {},
   "source": [
    "##### 3.Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7383949",
   "metadata": {},
   "source": [
    "There are many words that occur frequently like:is,the,and etc\n",
    "\n",
    "But in NLP pipeline we flag this words as stopwords\n",
    "\n",
    "Stopwords are those words in the text which does not add any meaning to the sentence and their removal will not affect our preprocessing of the text.\n",
    "\n",
    "Why do we remove this?They are removed from vocabulary to reduce the noise and also to reduce the dimension of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "986d375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a1d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f0fa5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "print(stopwords[0:10])\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da472dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "sample_data = [data for data in sample_data if data not in stopwords ]\n",
    "print(sample_data)\n",
    "print(len(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ba49a",
   "metadata": {},
   "source": [
    "After removing the stopwords it reduced to 6 from 9(see 11th cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223faaff",
   "metadata": {},
   "source": [
    "##### 4.Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54752cd",
   "metadata": {},
   "source": [
    "This is used to normalize the words into its base or root forms.\n",
    "\n",
    "One problem with stemming is that sometimes it produces the root word which may not have any meaning at all \n",
    "\n",
    "When doing data pre processing either we go with stemming or we go with lemmetization\n",
    "\n",
    "lemmetization is quite similar to stemming but it is used to group different inflected form of the words.It is called as lemma\n",
    "\n",
    "the main main difference between stemming and lemmetization is that it produce the root word which has a meaning.But when speed is compared stemming is faster than lemmetization.Because in lemmetization scan a corgpus which consumes time and the pre processing\n",
    "\n",
    "if building level application,then we go with lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3923f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# stemmig library used PorterStemmer\n",
    "# create a object for stemming\n",
    "ps = PorterStemmer()\n",
    "sample_data_stemming = [ps.stem(data) for data in sample_data]\n",
    "print(sample_data_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0731fe",
   "metadata": {},
   "source": [
    "##### 5.Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49bb94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4d15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c1ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# lemmetization library used WordNetLemmatizer\n",
    "# create word lemetizer objects\n",
    "lm = WordNetLemmatizer()\n",
    "sample_data_lemmatizer = [lm.lemmatize(data) for data in sample_data]\n",
    "print(sample_data_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658cfd5",
   "metadata": {},
   "source": [
    "**The above 5 operations are performed on our main dataset which is created with title and the label column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac6fe5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea36a378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392586a",
   "metadata": {},
   "source": [
    "Our for loop will be running for 18285 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27165d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now \n",
    "lm = WordNetLemmatizer()\n",
    "corpus = [] #empty list created with corpus object\n",
    "for i in range(len(df)):\n",
    "    review = re.sub('^a-zA-Z0-9',' ',df['title'][i])#regular expression\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lm.lemmatize(x) for x in review if x not in stopwords]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97103208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18285"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f7ff271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43c6e805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house dem aide: didn’t even see comey’s letter jason chaffetz tweeted'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a305855",
   "metadata": {},
   "source": [
    "### Vectorizer (Convert Text data into Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2930ada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "x = tf.fit_transform(corpus).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "618e36ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['label']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f669093",
   "metadata": {},
   "source": [
    "### Data Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e62a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=10,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3097d1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12799, 12799)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train),len(y_train)  #xtrain is input(independent variable) &&&&&& ytrain is output(dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13f5491e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5486, 5486)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test),len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30867a8c",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34348bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0dee36",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb785fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362012395187751"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "accuracyy = accuracy_score(y_test,y_pred)\n",
    "accuracyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75220b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self,model,x_train,x_test,y_train,y_test):\n",
    "        self.model = model\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    \n",
    "    def train_evaluation(self):\n",
    "        y_pred_train = self.model.predict(self.x_train)\n",
    "        \n",
    "        acc_scr_train = accuracy_score(self.y_train,y_pred_train)\n",
    "        print(\"Accuracy score on the training data set:\\n\",acc_scr_train)\n",
    "        print()\n",
    "        \n",
    "        con_mat_train = confusion_matrix(self.y_train,y_pred_train)\n",
    "        print(\"Confusion MAtrix on the training data set:\\n\",con_mat_train)\n",
    "        print()\n",
    "        \n",
    "        class_rep_train = classification_report(self.y_train,y_pred_train)\n",
    "        print(\"Classification Report on the training data set:\\n\",class_rep_train)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def test_evaluation(self):\n",
    "        y_pred_test = self.model.predict(self.x_test)\n",
    "        \n",
    "        acc_scr_test = accuracy_score(self.y_test,y_pred_test)\n",
    "        print(\"Accuracy score on the testing data set:\\n\",acc_scr_test)\n",
    "        print()\n",
    "        \n",
    "        con_mat_test = confusion_matrix(self.y_test,y_pred_test)\n",
    "        print(\"Confusion MAtrix on the testing data set:\\n\",con_mat_test)\n",
    "        print()\n",
    "        \n",
    "        class_rep_test = classification_report(self.y_test,y_pred_test)\n",
    "        print(\"Classification Report on the testing data set:\\n\",class_rep_test)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d56a4f",
   "metadata": {},
   "source": [
    "##### Checking Accuracy on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87e6b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the training data set:\n",
      " 1.0\n",
      "\n",
      "Confusion MAtrix on the training data set:\n",
      " [[7252    0]\n",
      " [   0 5547]]\n",
      "\n",
      "Classification Report on the training data set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7252\n",
      "           1       1.00      1.00      1.00      5547\n",
      "\n",
      "    accuracy                           1.00     12799\n",
      "   macro avg       1.00      1.00      1.00     12799\n",
      "weighted avg       1.00      1.00      1.00     12799\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Evaluation(rf,x_train,x_test,y_train,y_test).train_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9801f",
   "metadata": {},
   "source": [
    "#####  Checking Accuracy on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f10e3456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the testing data set:\n",
      " 0.9362012395187751\n",
      "\n",
      "Confusion MAtrix on the testing data set:\n",
      " [[2820  289]\n",
      " [  61 2316]]\n",
      "\n",
      "Classification Report on the testing data set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94      3109\n",
      "           1       0.89      0.97      0.93      2377\n",
      "\n",
      "    accuracy                           0.94      5486\n",
      "   macro avg       0.93      0.94      0.94      5486\n",
      "weighted avg       0.94      0.94      0.94      5486\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Evaluation(rf,x_train,x_test,y_train,y_test).test_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eac5e8",
   "metadata": {},
   "source": [
    "### Prediction Pipelining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edb390c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Preprocessing:\n",
    "#     def __init__(self,data):\n",
    "#         self.data = data\n",
    "    \n",
    "#     def text_preprocessing_user(self):\n",
    "#         lm = WordNetLemmatizer()\n",
    "#         pred_data = [self.data] \n",
    "#         preprocess_data = []\n",
    "#         for data in pred_data:\n",
    "#             review = re.sub('^a-zA-Z0-9',' ',data)\n",
    "#             review = review.lower()\n",
    "#             review = review.split()\n",
    "#             review = [lm.lemmatize(x) for x in review if x not in stopwords]\n",
    "#             review = \" \".join(review)\n",
    "#             preprocess_data.append(review)\n",
    "#         return preprocess_data\n",
    "    \n",
    "class Preprocessing:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def text_preprocessing_user(self):\n",
    "        lm = WordNetLemmatizer()\n",
    "        pred_data = [self.data]\n",
    "        preprocess_data = []\n",
    "        \n",
    "        # Use the correct regex pattern to remove unwanted characters\n",
    "        for data in pred_data:\n",
    "            review = re.sub('[^a-zA-Z0-9]', ' ', data)  # Corrected regex\n",
    "            review = review.lower()\n",
    "            review = review.split()\n",
    "            review = [lm.lemmatize(x) for x in review if x not in stopwords.words('english')]\n",
    "            review = \" \".join(review)\n",
    "            preprocess_data.append(review)\n",
    "        \n",
    "        return preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dddbaa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4808b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flynn: hillary clinton, big woman campus - breitbart']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'\n",
    "Preprocessing(data).text_preprocessing_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b68f5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Prediction:\n",
    "#     def __init__(self,pred_data,model):\n",
    "#         self.pred_data = pred_data\n",
    "#         self.model = model\n",
    "        \n",
    "#     def prediction_model(self):\n",
    "#         process_data = Preprocessing(self.pred_data).text_preprocessing_user\n",
    "#         data = tf.transform(preprocess_data)\n",
    "#         prediction = self.model.predict(data)\n",
    "        \n",
    "#         if prediction [0] == 0 :\n",
    "#             return \"The news is Fake\"\n",
    "#         else:\n",
    "#             return \"The news is Real\"\n",
    "        \n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self, pred_data, model):\n",
    "        self.pred_data = pred_data\n",
    "        self.model = model\n",
    "        \n",
    "    def prediction_model(self):\n",
    "        # Correct the method name here\n",
    "        preprocess_data = Preprocessing(self.pred_data).text_preprocessing_user()\n",
    "        \n",
    "        # Assuming tf is a TfidfVectorizer or similar\n",
    "        data = tf.transform(preprocess_data)  # Ensure tf is defined and initialized properly\n",
    "        prediction = self.model.predict(data)\n",
    "        \n",
    "        if prediction[0] == 0:\n",
    "            return \"The news is Fake\"\n",
    "        else:\n",
    "            return \"The news is Real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdd42e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The news is Fake'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'\n",
    "Prediction(data,rf).prediction_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c13e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
